# Conceptos de Performance y OptimizaciÃ³n en APIs

ğŸ“š **Fundamentos de optimizaciÃ³n para APIs en producciÃ³n**  
â° **Tiempo de lectura:** 30-45 minutos  
ğŸ¯ **Objetivo:** Comprender los principios fundamentales de performance

---

## ğŸ¯ Â¿QuÃ© es Performance en APIs?

### **DefiniciÃ³n**

**Performance en APIs** se refiere a quÃ© tan rÃ¡pido y eficientemente una API puede procesar requests y devolver responses, mientras utiliza los recursos del sistema de manera Ã³ptima.

### **MÃ©tricas Clave**

1. **Response Time** (Tiempo de Respuesta)

   - Tiempo desde request hasta response completa
   - Meta tÃ­pica: <200ms para operaciones CRUD bÃ¡sicas

2. **Throughput** (Rendimiento)

   - Requests procesados por segundo (RPS)
   - Meta tÃ­pica: >1000 RPS para APIs bien optimizadas

3. **Resource Usage** (Uso de Recursos)

   - CPU, Memoria, Disk I/O, Network I/O
   - Eficiencia en el uso de recursos disponibles

4. **Availability** (Disponibilidad)
   - Uptime del servicio
   - Meta tÃ­pica: 99.9% (8.77 horas down/aÃ±o)

---

## âš¡ Principios de OptimizaciÃ³n

### **1. Medir Antes de Optimizar**

```python
# Ejemplo: Profiling bÃ¡sico
import time
import functools

def timing_decorator(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        end = time.time()
        print(f"{func.__name__} took {end - start:.4f} seconds")
        return result
    return wrapper

@timing_decorator
def slow_endpoint():
    # Tu cÃ³digo aquÃ­
    pass
```

### **2. Ley de Pareto (80/20)**

- **80% de los problemas** vienen del **20% del cÃ³digo**
- **Identifica bottlenecks** antes de optimizar
- **Prioriza optimizaciones** con mayor impacto

### **3. OptimizaciÃ³n Prematura es Evil**

> "Premature optimization is the root of all evil" - Donald Knuth

- Primero funcionalidad, luego performance
- Optimiza solo bottlenecks identificados
- MantÃ©n cÃ³digo legible y mantenible

---

## ğŸ—‚ï¸ Estrategias de Caching

### **Â¿QuÃ© es Caching?**

**Caching** es el proceso de almacenar datos frecuentemente accedidos en una ubicaciÃ³n de acceso rÃ¡pido para reducir el tiempo de acceso en requests futuros.

### **Tipos de Cache**

#### **1. Application-Level Cache**

```python
# Cache en memoria simple
cache = {}

def get_user_cached(user_id: int):
    if user_id in cache:
        return cache[user_id]  # Cache hit

    user = database.get_user(user_id)  # Cache miss
    cache[user_id] = user
    return user
```

#### **2. Database Query Cache**

```python
# Con SQLAlchemy
from sqlalchemy.orm import sessionmaker

# Query cache automÃ¡tico
users = session.query(User).filter(User.active == True).all()
```

#### **3. External Cache (Redis)**

```python
import redis

redis_client = redis.Redis(host='localhost', port=6379, db=0)

def get_user_redis(user_id: int):
    # Intentar obtener del cache
    cached_user = redis_client.get(f"user:{user_id}")
    if cached_user:
        return json.loads(cached_user)

    # Si no estÃ¡ en cache, obtener de DB
    user = database.get_user(user_id)
    redis_client.setex(f"user:{user_id}", 300, json.dumps(user))  # TTL 5 min
    return user
```

### **Cache Patterns**

#### **1. Cache-Aside (Lazy Loading)**

```python
def get_product(product_id: int):
    # 1. Check cache first
    cached = redis_client.get(f"product:{product_id}")
    if cached:
        return json.loads(cached)

    # 2. If miss, load from database
    product = db.query(Product).filter(Product.id == product_id).first()

    # 3. Update cache
    if product:
        redis_client.setex(f"product:{product_id}", 600, json.dumps(product.dict()))

    return product
```

#### **2. Write-Through**

```python
def update_product(product_id: int, product_data: dict):
    # 1. Update database
    product = db.query(Product).filter(Product.id == product_id).first()
    for key, value in product_data.items():
        setattr(product, key, value)
    db.commit()

    # 2. Update cache immediately
    redis_client.setex(f"product:{product_id}", 600, json.dumps(product.dict()))

    return product
```

#### **3. Write-Behind (Write-Back)**

```python
# Cache se actualiza inmediatamente, DB se actualiza despuÃ©s
def update_product_writeback(product_id: int, product_data: dict):
    # 1. Update cache immediately
    redis_client.hset(f"product:{product_id}", mapping=product_data)

    # 2. Queue database update for later (background task)
    background_tasks.add_task(update_database, product_id, product_data)
```

### **Cache Invalidation**

```python
# Estrategias para invalidar cache
class CacheManager:
    def __init__(self):
        self.redis = redis.Redis()

    def invalidate_user_cache(self, user_id: int):
        """Invalidar cache especÃ­fico de usuario"""
        self.redis.delete(f"user:{user_id}")

    def invalidate_pattern(self, pattern: str):
        """Invalidar mÃºltiples keys por patrÃ³n"""
        keys = self.redis.keys(pattern)
        if keys:
            self.redis.delete(*keys)

    def set_with_tags(self, key: str, value: str, tags: list, ttl: int = 300):
        """Cache con tags para invalidaciÃ³n grupal"""
        self.redis.setex(key, ttl, value)
        for tag in tags:
            self.redis.sadd(f"tag:{tag}", key)

    def invalidate_by_tag(self, tag: str):
        """Invalidar todos los caches con un tag especÃ­fico"""
        keys = self.redis.smembers(f"tag:{tag}")
        if keys:
            self.redis.delete(*keys)
            self.redis.delete(f"tag:{tag}")
```

---

## ğŸ—ƒï¸ OptimizaciÃ³n de Base de Datos

### **1. Query Optimization**

#### **N+1 Problem**

```python
# âŒ Problema N+1 (1 query + N queries)
users = session.query(User).all()  # 1 query
for user in users:
    print(user.posts)  # N queries (una por cada user)

# âœ… SoluciÃ³n: Eager Loading
users = session.query(User).options(joinedload(User.posts)).all()  # 1 query
for user in users:
    print(user.posts)  # Sin queries adicionales
```

#### **Query Analysis con EXPLAIN**

```sql
-- Analizar performance de query
EXPLAIN ANALYZE SELECT * FROM users WHERE email = 'user@example.com';

-- Ejemplo de output:
-- Seq Scan on users (cost=0.00..25.00 rows=1000 width=32) (actual time=0.123..5.456 rows=1 loops=1)
--   Filter: (email = 'user@example.com'::text)
-- Planning Time: 0.234 ms
-- Execution Time: 5.678 ms
```

### **2. Database Indexes**

#### **Tipos de Ãndices**

```sql
-- Ãndice simple
CREATE INDEX idx_users_email ON users(email);

-- Ãndice compuesto
CREATE INDEX idx_users_status_created ON users(status, created_at);

-- Ãndice parcial
CREATE INDEX idx_active_users_email ON users(email) WHERE status = 'active';

-- Ãndice Ãºnico
CREATE UNIQUE INDEX idx_users_email_unique ON users(email);
```

#### **Estrategias de Indexing**

```python
# En SQLAlchemy models
class User(Base):
    __tablename__ = "users"

    id = Column(Integer, primary_key=True, index=True)
    email = Column(String, unique=True, index=True)  # Ãndice Ãºnico
    status = Column(String, index=True)  # Ãndice simple
    created_at = Column(DateTime, index=True)

    # Ãndice compuesto
    __table_args__ = (
        Index('idx_user_status_created', 'status', 'created_at'),
        Index('idx_active_users', 'email', postgresql_where=Column('status') == 'active'),
    )
```

### **3. Connection Pooling**

```python
# ConfiguraciÃ³n de connection pool
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool

engine = create_engine(
    "postgresql://user:pass@localhost/dbname",
    poolclass=QueuePool,
    pool_size=20,          # Conexiones en el pool
    max_overflow=30,       # Conexiones adicionales si es necesario
    pool_pre_ping=True,    # Verificar conexiones antes de usar
    pool_recycle=3600,     # Reciclar conexiones cada hora
)
```

### **4. Async Database Operations**

```python
# Operaciones asÃ­ncronas para mejor throughput
import asyncio
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine

async_engine = create_async_engine(
    "postgresql+asyncpg://user:pass@localhost/dbname"
)

async def get_users_async(db: AsyncSession, limit: int = 100):
    result = await db.execute(
        select(User).limit(limit)
    )
    return result.scalars().all()

# Operaciones concurrentes
async def get_multiple_resources():
    async with AsyncSession(async_engine) as session:
        users_task = asyncio.create_task(get_users_async(session))
        products_task = asyncio.create_task(get_products_async(session))

        users, products = await asyncio.gather(users_task, products_task)
        return users, products
```

---

## ğŸ› ï¸ Middleware para Performance

### **1. Rate Limiting**

```python
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

@app.get("/api/users")
@limiter.limit("10/minute")  # 10 requests por minuto
async def get_users(request: Request):
    return {"users": []}
```

### **2. Response Compression**

```python
from fastapi.middleware.gzip import GZipMiddleware

# Comprimir responses automÃ¡ticamente
app.add_middleware(GZipMiddleware, minimum_size=1000)
```

### **3. Custom Performance Middleware**

```python
import time
from fastapi import Request, Response

@app.middleware("http")
async def performance_middleware(request: Request, call_next):
    start_time = time.time()

    # Procesar request
    response = await call_next(request)

    # Calcular tiempo de procesamiento
    process_time = time.time() - start_time
    response.headers["X-Process-Time"] = str(process_time)

    # Log performance metrics
    logger.info(f"{request.method} {request.url.path} - {process_time:.4f}s")

    return response
```

---

## ğŸ“Š Monitoring y Profiling

### **1. Structured Logging**

```python
import structlog

# Configurar structured logging
structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="ISO"),
        structlog.processors.add_log_level,
        structlog.processors.JSONRenderer()
    ],
    wrapper_class=structlog.make_filtering_bound_logger(20),
    logger_factory=structlog.PrintLoggerFactory(),
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()

# Uso en endpoints
@app.get("/api/users/{user_id}")
async def get_user(user_id: int):
    logger.info("fetching_user", user_id=user_id)

    try:
        user = await get_user_from_db(user_id)
        logger.info("user_fetched", user_id=user_id, execution_time=0.234)
        return user
    except Exception as e:
        logger.error("user_fetch_failed", user_id=user_id, error=str(e))
        raise
```

### **2. Performance Profiling**

```python
# Profiling con cProfile
import cProfile
import pstats

def profile_endpoint():
    profiler = cProfile.Profile()
    profiler.enable()

    # Tu cÃ³digo aquÃ­
    result = expensive_operation()

    profiler.disable()
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    stats.print_stats(10)  # Top 10 funciones mÃ¡s lentas

    return result
```

### **3. Memory Profiling**

```python
from memory_profiler import profile

@profile
def memory_intensive_function():
    # FunciÃ³n que consume mucha memoria
    large_list = [i for i in range(1000000)]
    return large_list

# Uso: python -m memory_profiler script.py
```

### **4. Application Metrics**

```python
# MÃ©tricas bÃ¡sicas de aplicaciÃ³n
class MetricsCollector:
    def __init__(self):
        self.request_count = 0
        self.error_count = 0
        self.response_times = []

    def record_request(self, response_time: float, status_code: int):
        self.request_count += 1
        self.response_times.append(response_time)

        if status_code >= 400:
            self.error_count += 1

    def get_stats(self):
        avg_response_time = sum(self.response_times) / len(self.response_times)
        error_rate = (self.error_count / self.request_count) * 100

        return {
            "total_requests": self.request_count,
            "avg_response_time": avg_response_time,
            "error_rate": error_rate,
            "errors": self.error_count
        }

metrics = MetricsCollector()

@app.middleware("http")
async def metrics_middleware(request: Request, call_next):
    start_time = time.time()
    response = await call_next(request)
    response_time = time.time() - start_time

    metrics.record_request(response_time, response.status_code)
    return response
```

---

## ğŸ¯ Mejores PrÃ¡cticas

### **1. Caching Guidelines**

- âœ… **Cache data that doesn't change frequently**
- âœ… **Set appropriate TTL** (Time To Live)
- âœ… **Handle cache failures gracefully**
- âœ… **Monitor cache hit/miss ratios**
- âŒ **Don't cache user-specific sensitive data**
- âŒ **Don't cache data that changes frequently**

### **2. Database Optimization**

- âœ… **Use indexes on frequently queried columns**
- âœ… **Avoid N+1 queries with eager loading**
- âœ… **Use connection pooling**
- âœ… **Analyze slow queries with EXPLAIN**
- âŒ **Don't over-index (impacts write performance)**
- âŒ **Don't fetch unnecessary columns**

### **3. API Design for Performance**

- âœ… **Implement pagination for large datasets**
- âœ… **Use appropriate HTTP status codes**
- âœ… **Compress responses when beneficial**
- âœ… **Implement proper error handling**
- âŒ **Don't return massive payloads**
- âŒ **Don't expose internal implementation details**

### **4. Monitoring and Alerting**

- âœ… **Log structured data for analysis**
- âœ… **Monitor key performance metrics**
- âœ… **Set up alerting for critical thresholds**
- âœ… **Profile production performance regularly**
- âŒ **Don't log sensitive information**
- âŒ **Don't ignore error patterns**

---

## ğŸš¨ Performance Anti-Patterns

### **1. Over-Caching**

```python
# âŒ Bad: Cachear todo sin criterio
def bad_caching():
    cache.set("user_data", user_data, ttl=86400)  # 24 hours for sensitive data
    cache.set("random_number", random.random(), ttl=3600)  # Caching random data

# âœ… Good: Cache estratÃ©gico
def good_caching():
    # Solo cache datos costosos de obtener y que no cambian frecuentemente
    if not cache.exists("expensive_calculation"):
        result = expensive_database_aggregation()
        cache.set("expensive_calculation", result, ttl=1800)  # 30 min
```

### **2. Premature Optimization**

```python
# âŒ Bad: Optimizar sin medir
def premature_optimization():
    # CÃ³digo complejo para "performance" sin evidencia de que sea necesario
    result = complex_algorithm_that_might_be_faster()

# âœ… Good: Medir primero, optimizar despuÃ©s
@timing_decorator
def measured_approach():
    # ImplementaciÃ³n simple primero
    result = simple_algorithm()
    # Solo optimizar si el profiling muestra que es un bottleneck
```

### **3. Ignoring Database Performance**

```python
# âŒ Bad: Queries ineficientes
def bad_queries():
    users = session.query(User).all()  # Obtener todos los usuarios
    for user in users:
        if user.status == 'active':  # Filtrar en Python
            print(user.email)

# âœ… Good: Queries optimizadas
def good_queries():
    active_users = session.query(User).filter(User.status == 'active').all()
    for user in active_users:
        print(user.email)
```

---

## ğŸ“ PrÃ³ximos Pasos

DespuÃ©s de dominar estos conceptos fundamentales:

1. **Implementa caching** en tu API siguiendo los patrones aprendidos
2. **Optimiza tus queries** mÃ¡s lentas usando EXPLAIN
3. **Configura monitoring** bÃ¡sico para mÃ©tricas clave
4. **Aplica middleware** para rate limiting y compresiÃ³n
5. **Perfila tu aplicaciÃ³n** para identificar bottlenecks reales

Â¡En las prÃ¡cticas implementaremos cada uno de estos conceptos de manera prÃ¡ctica! âš¡
