# .github/workflows/bc-fastapi-evaluation.yml
# GitHub Action optimizado para evaluaciÃ³n automÃ¡tica del bootcamp bc-fastapi
# VersiÃ³n corregida y mejorada

name: ğŸ¯ EvaluaciÃ³n AutomÃ¡tica bc-fastapi

on:
  repository_dispatch:
    types: [student-submission]
  workflow_dispatch:
    inputs:
      student_repo:
        description: 'Repositorio del estudiante (usuario/repo)'
        required: true
        type: string
      pr_number:
        description: 'NÃºmero del Pull Request'
        required: true
        type: string
      week_number:
        description: 'NÃºmero de semana (1-11)'
        required: false
        type: string
        default: '1'

env:
  PYTHON_VERSION: "3.11"
  EVALUATION_TIMEOUT: "15m"

permissions:
  contents: read
  actions: write

jobs:
  # Job principal: EvaluaciÃ³n completa
  evaluate-submission:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
    - name: ğŸ“¥ Checkout repositorio bc-fastapi
      uses: actions/checkout@v4
      with:
        repository: ${{ github.repository }}
        path: bc-fastapi-main
        
    - name: ğŸ” Parsear contexto de evaluaciÃ³n
      id: context
      run: |
        # Determina origen de la evaluaciÃ³n
        if [ "${{ github.event_name }}" = "repository_dispatch" ]; then
          STUDENT_REPO="${{ github.event.client_payload.repository }}"
          PR_NUMBER="${{ github.event.client_payload.pr_number }}"
          BRANCH_NAME="${{ github.event.client_payload.branch_name }}"
          WEEK_NUMBER="${{ github.event.client_payload.week_number }}"
        else
          STUDENT_REPO="${{ github.event.inputs.student_repo }}"
          PR_NUMBER="${{ github.event.inputs.pr_number }}"
          BRANCH_NAME="semana-${{ github.event.inputs.week_number }}-entrega"
          WEEK_NUMBER="${{ github.event.inputs.week_number }}"
        fi
        
        STUDENT_NAME=$(echo "$STUDENT_REPO" | cut -d'/' -f1)
        
        echo "student_repo=$STUDENT_REPO" >> $GITHUB_OUTPUT
        echo "student_name=$STUDENT_NAME" >> $GITHUB_OUTPUT
        echo "pr_number=$PR_NUMBER" >> $GITHUB_OUTPUT
        echo "branch_name=$BRANCH_NAME" >> $GITHUB_OUTPUT
        echo "week_number=${WEEK_NUMBER:-1}" >> $GITHUB_OUTPUT
        
        echo "ğŸ“Š Contexto:"
        echo "   ğŸ‘¤ Estudiante: $STUDENT_NAME"
        echo "   ğŸ“ Repo: $STUDENT_REPO"
        echo "   ğŸ”€ PR: #$PR_NUMBER"
        echo "   ğŸ“… Semana: ${WEEK_NUMBER:-1}"

    - name: ğŸ“¥ Clonar repositorio del estudiante
      run: |
        STUDENT_REPO="${{ steps.context.outputs.student_repo }}"
        BRANCH_NAME="${{ steps.context.outputs.branch_name }}"
        
        echo "ğŸ”„ Clonando $STUDENT_REPO..."
        git clone "https://github.com/$STUDENT_REPO.git" student-repo || {
          echo "âŒ Error: No se pudo clonar el repositorio"
          exit 1
        }
        
        cd student-repo
        
        # Intenta checkout a la rama especÃ­fica
        if git branch -r | grep -q "origin/$BRANCH_NAME"; then
          git checkout "$BRANCH_NAME"
          echo "âœ… Rama $BRANCH_NAME activada"
        else
          echo "âš ï¸ Rama $BRANCH_NAME no encontrada, usando main/master"
        fi
        
        echo "ğŸ“Š Archivos Python encontrados:"
        find . -name "*.py" -type f | head -5

    - name: ğŸ Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: ğŸ“¦ Instalar dependencias bÃ¡sicas
      run: |
        pip install --upgrade pip
        pip install requests pydantic fastapi
        pip install flake8 radon
        echo "âœ… Dependencias instaladas"

    - name: ğŸ” Analizar cÃ³digo del estudiante
      id: analysis
      run: |
        cd student-repo
        
        # Script de anÃ¡lisis de cÃ³digo
        python << 'EOF'
        import os
        import json
        import re
        from pathlib import Path
        
        def analyze_student_code():
            analysis = {
                "files_analyzed": [],
                "total_lines": 0,
                "functions_count": 0,
                "classes_count": 0,
                "fastapi_usage": False,
                "endpoints_found": [],
                "test_files": [],
                "has_main": False,
                "syntax_errors": []
            }
            
            for py_file in Path('.').rglob('*.py'):
                if '.git' in str(py_file) or '__pycache__' in str(py_file):
                    continue
                    
                try:
                    with open(py_file, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                    
                    analysis["files_analyzed"].append(str(py_file))
                    lines = content.splitlines()
                    analysis["total_lines"] += len([l for l in lines if l.strip()])
                    
                    # AnÃ¡lisis de patrones
                    if 'fastapi' in content.lower() or 'FastAPI' in content:
                        analysis["fastapi_usage"] = True
                    
                    if '__name__ == "__main__"' in content:
                        analysis["has_main"] = True
                    
                    # Busca endpoints
                    endpoint_patterns = [
                        r'@app\.(get|post|put|delete|patch)\s*\(\s*["\']([^"\']+)["\']',
                        r'@router\.(get|post|put|delete|patch)\s*\(\s*["\']([^"\']+)["\']'
                    ]
                    
                    for pattern in endpoint_patterns:
                        matches = re.finditer(pattern, content, re.IGNORECASE)
                        for match in matches:
                            method = match.group(1).upper()
                            path = match.group(2)
                            analysis["endpoints_found"].append(f"{method} {path}")
                    
                    # Cuenta funciones y clases
                    analysis["functions_count"] += len(re.findall(r'^\s*def\s+\w+', content, re.MULTILINE))
                    analysis["classes_count"] += len(re.findall(r'^\s*class\s+\w+', content, re.MULTILINE))
                    
                    # Identifica tests
                    if 'test' in py_file.name.lower():
                        analysis["test_files"].append(str(py_file))
                        
                except Exception as e:
                    analysis["syntax_errors"].append(f"{py_file}: {str(e)}")
            
            return analysis
        
        analysis = analyze_student_code()
        
        # Guarda anÃ¡lisis
        with open('../analysis_result.json', 'w') as f:
            json.dump(analysis, f, indent=2)
        
        print(f"ğŸ“Š AnÃ¡lisis completado:")
        print(f"   ğŸ“„ Archivos: {len(analysis['files_analyzed'])}")
        print(f"   ğŸ“ LÃ­neas: {analysis['total_lines']}")
        print(f"   ğŸš€ FastAPI: {'SÃ­' if analysis['fastapi_usage'] else 'No'}")
        print(f"   ğŸ¯ Endpoints: {len(analysis['endpoints_found'])}")
        EOF

    - name: ğŸ“‹ Generar evaluaciÃ³n bÃ¡sica
      run: |
        STUDENT_NAME="${{ steps.context.outputs.student_name }}"
        WEEK_NUMBER="${{ steps.context.outputs.week_number }}"
        
        # Carga anÃ¡lisis
        ANALYSIS=$(cat analysis_result.json)
        
        # Genera evaluaciÃ³n bÃ¡sica
        python << 'EOF'
        import json
        import os
        from datetime import datetime
        
        # Carga contexto
        student_name = os.getenv('STUDENT_NAME', 'Estudiante')
        week_number = int(os.getenv('WEEK_NUMBER', '1'))
        
        # Carga anÃ¡lisis
        with open('analysis_result.json', 'r') as f:
            analysis = json.load(f)
        
        # Genera evaluaciÃ³n
        def generate_evaluation():
            total_files = len(analysis['files_analyzed'])
            total_lines = analysis['total_lines']
            has_fastapi = analysis['fastapi_usage']
            endpoints_count = len(analysis['endpoints_found'])
            
            # LÃ³gica de evaluaciÃ³n bÃ¡sica
            score = 0
            feedback_points = []
            improvements = []
            
            # Criterio 1: Entrega de cÃ³digo (20 pts)
            if total_files > 0:
                score += 20
                feedback_points.append("âœ… CÃ³digo Python entregado correctamente")
            else:
                improvements.append("âŒ No se encontraron archivos Python en la entrega")
            
            # Criterio 2: Uso de FastAPI (30 pts)
            if has_fastapi:
                score += 30
                feedback_points.append("âœ… FastAPI detectado en el cÃ³digo")
            else:
                score += 10  # Puntos parciales por intentar
                improvements.append("âš ï¸ No se detectÃ³ uso de FastAPI - Revisa las importaciones")
            
            # Criterio 3: ImplementaciÃ³n de endpoints (25 pts)
            if endpoints_count >= 3:
                score += 25
                feedback_points.append(f"âœ… {endpoints_count} endpoints implementados correctamente")
            elif endpoints_count > 0:
                score += 15
                feedback_points.append(f"âš ï¸ {endpoints_count} endpoints encontrados - Se esperan al menos 3")
                improvements.append("Implementa mÃ¡s endpoints segÃºn los requisitos de la semana")
            else:
                improvements.append("âŒ No se encontraron endpoints - Revisa la implementaciÃ³n de rutas")
            
            # Criterio 4: Estructura y calidad (25 pts)
            if total_lines >= 50:
                score += 20
                feedback_points.append("âœ… CÃ³digo con estructura adecuada")
            elif total_lines >= 20:
                score += 15
                feedback_points.append("âš ï¸ CÃ³digo bÃ¡sico implementado")
            else:
                improvements.append("âŒ CÃ³digo muy bÃ¡sico - Expande la implementaciÃ³n")
            
            if analysis['functions_count'] >= 3:
                score += 5
                feedback_points.append("âœ… Buena modularizaciÃ³n con funciones")
            
            # Determina categorÃ­a
            if score >= 90:
                category = "Excelente"
                emoji = "ğŸ†"
            elif score >= 80:
                category = "Satisfactorio"
                emoji = "âœ…"
            elif score >= 70:
                category = "Necesita Mejora"
                emoji = "âš ï¸"
            else:
                category = "Insuficiente"
                emoji = "âŒ"
            
            return score, category, emoji, feedback_points, improvements
        
        score, category, emoji, feedback_points, improvements = generate_evaluation()
        
        # Genera reporte
        report = f'''# ğŸ¯ EvaluaciÃ³n AutomÃ¡tica bc-fastapi

**Estudiante:** {student_name}  
**Semana:** {week_number}  
**Fecha:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  

---

## {emoji} CalificaciÃ³n: {score}/100 puntos ({category})

### âœ… Fortalezas Identificadas:
'''
        
        for point in feedback_points:
            report += f"- {point}\n"
        
        if improvements:
            report += "\n### ğŸ¯ Ãreas de Mejora:\n"
            for improvement in improvements:
                report += f"- {improvement}\n"
        
        report += f'''

### ğŸ“Š AnÃ¡lisis TÃ©cnico:
- **Archivos analizados:** {len(analysis["files_analyzed"])}
- **LÃ­neas de cÃ³digo:** {analysis["total_lines"]}
- **Funciones:** {analysis["functions_count"]}
- **FastAPI detectado:** {"âœ…" if analysis["fastapi_usage"] else "âŒ"}
- **Endpoints:** {len(analysis["endpoints_found"])}

### ğŸ“š PrÃ³ximos Pasos para Semana {week_number + 1}:
- Revisa el material de la siguiente semana en el repositorio principal
- Implementa las mejoras sugeridas si las hay
- Participa en las discusiones del curso para resolver dudas

### ğŸ“ Recursos de Apoyo:
- **Repositorio principal:** https://github.com/elparchetipk/bc-fastapi
- **Material de la semana:** `/semana-{week_number:02d}/`
- **DocumentaciÃ³n:** `/_docs/`

---

## ğŸ¤– InformaciÃ³n de la EvaluaciÃ³n

- **Sistema:** EvaluaciÃ³n automÃ¡tica bc-fastapi v2.0
- **Algoritmo:** AnÃ¡lisis estÃ¡tico de cÃ³digo + ValidaciÃ³n de patrones
- **Instructor:** TTCO
- **InstituciÃ³n:** SENA - CGMLTI Regional Distrito Capital

---

*EvaluaciÃ³n generada automÃ¡ticamente el {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}*
'''
        
        # Guarda reporte
        with open('evaluation_report.md', 'w', encoding='utf-8') as f:
            f.write(report)
        
        print("âœ… EvaluaciÃ³n generada exitosamente")
        print(f"ğŸ“Š PuntuaciÃ³n: {score}/100 ({category})")
        
        EOF
        
        env:
          STUDENT_NAME: ${{ steps.context.outputs.student_name }}
          WEEK_NUMBER: ${{ steps.context.outputs.week_number }}

    - name: ğŸ“¤ Subir reporte de evaluaciÃ³n
      uses: actions/upload-artifact@v4
      with:
        name: evaluation-report-${{ steps.context.outputs.student_name }}-week-${{ steps.context.outputs.week_number }}
        path: |
          evaluation_report.md
          analysis_result.json
        retention-days: 90

    - name: ğŸ“ Mostrar resumen de evaluaciÃ³n
      run: |
        echo "ğŸ¯ RESUMEN DE EVALUACIÃ“N"
        echo "========================"
        echo "ğŸ‘¤ Estudiante: ${{ steps.context.outputs.student_name }}"
        echo "ğŸ“… Semana: ${{ steps.context.outputs.week_number }}"
        echo "ğŸ“ Repositorio: ${{ steps.context.outputs.student_repo }}"
        echo ""
        echo "ğŸ“Š RESULTADO:"
        if [ -f "evaluation_report.md" ]; then
          echo "âœ… EvaluaciÃ³n completada exitosamente"
          # Extrae la puntuaciÃ³n del reporte
          grep "CalificaciÃ³n:" evaluation_report.md || echo "ğŸ“Š Ver artifact para detalles"
        else
          echo "âŒ Error en la evaluaciÃ³n"
        fi

  # Job opcional: NotificaciÃ³n por issue (requiere token)
  create-feedback-issue:
    needs: evaluate-submission
    if: ${{ success() && vars.ENABLE_ISSUE_FEEDBACK == 'true' }}
    runs-on: ubuntu-latest
    timeout-minutes: 5
    
    steps:
    - name: â¬‡ï¸ Descargar reporte de evaluaciÃ³n
      uses: actions/download-artifact@v4
      with:
        name: evaluation-report-${{ needs.evaluate-submission.steps.context.outputs.student_name }}-week-${{ needs.evaluate-submission.steps.context.outputs.week_number }}

    - name: ğŸ“ Crear issue de feedback (opcional)
      if: ${{ secrets.STUDENT_REPOS_TOKEN != '' }}
      uses: actions/github-script@v7
      with:
        github-token: ${{ secrets.STUDENT_REPOS_TOKEN }}
        script: |
          const fs = require('fs');
          
          try {
            const evaluation = fs.readFileSync('evaluation_report.md', 'utf8');
            const studentRepo = '${{ needs.evaluate-submission.steps.context.outputs.student_repo }}';
            const weekNumber = '${{ needs.evaluate-submission.steps.context.outputs.week_number }}';
            
            const [owner, repo] = studentRepo.split('/');
            
            const issue = await github.rest.issues.create({
              owner: owner,
              repo: repo,
              title: `ğŸ“‹ EvaluaciÃ³n Semana ${weekNumber} - Feedback Automatizado`,
              body: evaluation + '\n\n---\n\nğŸ¤– *Issue creado automÃ¡ticamente por el sistema bc-fastapi*',
              labels: ['evaluacion-automatica', `semana-${weekNumber}`, 'bc-fastapi']
            });
            
            console.log(`âœ… Issue creado: ${issue.data.html_url}`);
          } catch (error) {
            console.log(`âš ï¸ No se pudo crear issue: ${error.message}`);
          }
